/*
 * IDENTIFICATION:
 * stub generated by bootstrap_cmds-138
 * OPTIONS: 
 *	KernelServer
 */

/* Module map */

#define	__MIG_check__Request__map_subsystem__ 1

#include "vm32_map_server.h"

#ifndef	mig_internal
#define	mig_internal	static __inline__
#endif	/* mig_internal */

#ifndef	mig_external
#define mig_external
#endif	/* mig_external */

#if	!defined(__MigTypeCheck) && defined(TypeCheck)
#define	__MigTypeCheck		TypeCheck	/* Legacy setting */
#endif	/* !defined(__MigTypeCheck) */

#if	!defined(__MigKernelSpecificCode) && defined(_MIG_KERNEL_SPECIFIC_CODE_)
#define	__MigKernelSpecificCode	_MIG_KERNEL_SPECIFIC_CODE_	/* Legacy setting */
#endif	/* !defined(__MigKernelSpecificCode) */

#ifndef	LimitCheck
#define	LimitCheck 0
#endif	/* LimitCheck */

#ifndef	min
#define	min(a,b)  ( ((a) < (b))? (a): (b) )
#endif	/* min */

#if !defined(_WALIGN_)
#define _WALIGN_(x) (((x) + 3) & ~3)
#endif /* !defined(_WALIGN_) */

#if !defined(_WALIGNSZ_)
#define _WALIGNSZ_(x) _WALIGN_(sizeof(x))
#endif /* !defined(_WALIGNSZ_) */

#ifndef	UseStaticTemplates
#define	UseStaticTemplates	0
#endif	/* UseStaticTemplates */

#ifndef MIG_SERVER_ROUTINE
#define MIG_SERVER_ROUTINE
#endif

#ifndef	__DeclareRcvRpc
#define	__DeclareRcvRpc(_NUM_, _NAME_)
#endif	/* __DeclareRcvRpc */

#ifndef	__BeforeRcvRpc
#define	__BeforeRcvRpc(_NUM_, _NAME_)
#endif	/* __BeforeRcvRpc */

#ifndef	__AfterRcvRpc
#define	__AfterRcvRpc(_NUM_, _NAME_)
#endif	/* __AfterRcvRpc */

#ifndef	__DeclareRcvSimple
#define	__DeclareRcvSimple(_NUM_, _NAME_)
#endif	/* __DeclareRcvSimple */

#ifndef	__BeforeRcvSimple
#define	__BeforeRcvSimple(_NUM_, _NAME_)
#endif	/* __BeforeRcvSimple */

#ifndef	__AfterRcvSimple
#define	__AfterRcvSimple(_NUM_, _NAME_)
#endif	/* __AfterRcvSimple */

#define novalue void

#if	__MigKernelSpecificCode
#define msgh_request_port	msgh_remote_port
#define MACH_MSGH_BITS_REQUEST(bits)	MACH_MSGH_BITS_REMOTE(bits)
#define msgh_reply_port		msgh_local_port
#define MACH_MSGH_BITS_REPLY(bits)	MACH_MSGH_BITS_LOCAL(bits)
#else
#define msgh_request_port	msgh_local_port
#define MACH_MSGH_BITS_REQUEST(bits)	MACH_MSGH_BITS_LOCAL(bits)
#define msgh_reply_port		msgh_remote_port
#define MACH_MSGH_BITS_REPLY(bits)	MACH_MSGH_BITS_REMOTE(bits)
#endif /* __MigKernelSpecificCode */

#define MIG_RETURN_ERROR(X, code)	{\
				((mig_reply_error_t *)X)->RetCode = code;\
				((mig_reply_error_t *)X)->NDR = NDR_record;\
				return;\
				}

/* Forward Declarations */


mig_internal novalue _Xregion
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xallocate
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xdeallocate
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xprotect
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xinherit
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xread
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xread_list
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xwrite
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xcopy
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xread_overwrite
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xmsync
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xbehavior_set
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xmap
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xmachine_attribute
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xremap
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _X_task_wire
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xmake_memory_entry
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xmap_page_query
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xregion_info
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xmapped_pages_info
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xregion_recurse
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xregion_recurse_64
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xregion_info_64
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xregion_64
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xmake_memory_entry_64
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xmap_64
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _Xpurgable_control
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);

mig_internal novalue _X_map_exec_lockdown
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP);


#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__region_t__defined)
#define __MIG_check__Request__region_t__defined

mig_internal kern_return_t __MIG_check__Request__region_t(
	__attribute__((__unused__)) __RequestKData__region_t *InKP,
	__attribute__((__unused__)) __RequestUData__region_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__region_t __Request;
	typedef __RequestUData__region_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__region_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine region */
mig_internal novalue _Xregion
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_t address;
		vm_region_flavor_t flavor;
		mach_msg_type_number_t infoCnt;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__region_t RequestK;
	typedef __RequestUData__region_t __RequestU;
	typedef __ReplyKData__region_t ReplyK __attribute__((unused));
	typedef __ReplyUData__region_t ReplyU __attribute__((unused));
	typedef __Reply__region_t Reply __attribute__((unused));
	typedef __Request__region_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__region_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__region_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_nameTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_nameTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_read_t target_task;
	mach_port_t object_name;

	__DeclareRcvRpc(3800, "region")
	__BeforeRcvRpc(3800, "region")

#if	defined(__MIG_check__Request__region_t__defined)
	check_result = __MIG_check__Request__region_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__region_t__defined) */

#if	UseStaticTemplates
	OutKP->object_name = object_nameTemplate;
#else	/* UseStaticTemplates */
#if __MigKernelSpecificCode
	OutKP->object_name.disposition = 17;
#else
	OutKP->object_name.disposition = 17;
#endif /* __MigKernelSpecificCode */
#if !(defined(KERNEL) && defined(__LP64__))
	OutKP->object_name.pad1 = 0;
#endif
	OutKP->object_name.pad2 = 0;
	OutKP->object_name.type = MACH_MSG_PORT_DESCRIPTOR;
#if defined(KERNEL)
	OutKP->object_name.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	target_task = convert_port_to_map_read(InKP->Head.msgh_request_port);

	OutUP->infoCnt = 10;
	if (In0UP->infoCnt < OutUP->infoCnt)
		OutUP->infoCnt = In0UP->infoCnt;

	RetCode = vm32_region(target_task, &In0UP->address, &OutUP->size, In0UP->flavor, OutUP->info, &OutUP->infoCnt, &object_name);
	vm_map_read_deallocate(target_task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */
	OutKP->object_name.name = object_name;


	OutUP->NDR = NDR_record;


	OutUP->address = In0UP->address;
	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply) - 40) + (((4 * OutUP->infoCnt)));

	OutKP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutKP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3800, "region")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__allocate_t__defined)
#define __MIG_check__Request__allocate_t__defined

mig_internal kern_return_t __MIG_check__Request__allocate_t(
	__attribute__((__unused__)) __RequestKData__allocate_t *InKP,
	__attribute__((__unused__)) __RequestUData__allocate_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__allocate_t __Request;
	typedef __RequestUData__allocate_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__allocate_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine allocate */
mig_internal novalue _Xallocate
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_ut address;
		vm32_size_ut size;
		int flags;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__allocate_t RequestK;
	typedef __RequestUData__allocate_t __RequestU;
	typedef __ReplyKData__allocate_t ReplyK __attribute__((unused));
	typedef __ReplyUData__allocate_t ReplyU __attribute__((unused));
	typedef __Reply__allocate_t Reply __attribute__((unused));
	typedef __Request__allocate_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__allocate_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__allocate_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3801, "allocate")
	__BeforeRcvRpc(3801, "allocate")

#if	defined(__MIG_check__Request__allocate_t__defined)
	check_result = __MIG_check__Request__allocate_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__allocate_t__defined) */

	target_task = convert_port_entry_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_allocate(target_task, &In0UP->address, In0UP->size, In0UP->flags);
	vm_map_deallocate(target_task);
	if (OutUP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, OutUP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	OutUP->address = In0UP->address;

	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3801, "allocate")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__deallocate_t__defined)
#define __MIG_check__Request__deallocate_t__defined

mig_internal kern_return_t __MIG_check__Request__deallocate_t(
	__attribute__((__unused__)) __RequestKData__deallocate_t *InKP,
	__attribute__((__unused__)) __RequestUData__deallocate_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__deallocate_t __Request;
	typedef __RequestUData__deallocate_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__deallocate_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine deallocate */
mig_internal novalue _Xdeallocate
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_ut address;
		vm32_size_ut size;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__deallocate_t RequestK;
	typedef __RequestUData__deallocate_t __RequestU;
	typedef __ReplyKData__deallocate_t ReplyK __attribute__((unused));
	typedef __ReplyUData__deallocate_t ReplyU __attribute__((unused));
	typedef __Reply__deallocate_t Reply __attribute__((unused));
	typedef __Request__deallocate_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__deallocate_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__deallocate_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3802, "deallocate")
	__BeforeRcvRpc(3802, "deallocate")

#if	defined(__MIG_check__Request__deallocate_t__defined)
	check_result = __MIG_check__Request__deallocate_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__deallocate_t__defined) */

	target_task = convert_port_entry_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_deallocate(target_task, In0UP->address, In0UP->size);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	__AfterRcvRpc(3802, "deallocate")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__protect_t__defined)
#define __MIG_check__Request__protect_t__defined

mig_internal kern_return_t __MIG_check__Request__protect_t(
	__attribute__((__unused__)) __RequestKData__protect_t *InKP,
	__attribute__((__unused__)) __RequestUData__protect_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__protect_t __Request;
	typedef __RequestUData__protect_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__protect_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine protect */
mig_internal novalue _Xprotect
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		boolean_t set_maximum;
		vm_prot_t new_protection;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__protect_t RequestK;
	typedef __RequestUData__protect_t __RequestU;
	typedef __ReplyKData__protect_t ReplyK __attribute__((unused));
	typedef __ReplyUData__protect_t ReplyU __attribute__((unused));
	typedef __Reply__protect_t Reply __attribute__((unused));
	typedef __Request__protect_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__protect_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__protect_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3803, "protect")
	__BeforeRcvRpc(3803, "protect")

#if	defined(__MIG_check__Request__protect_t__defined)
	check_result = __MIG_check__Request__protect_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__protect_t__defined) */

	target_task = convert_port_entry_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_protect(target_task, In0UP->address, In0UP->size, In0UP->set_maximum, In0UP->new_protection);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	__AfterRcvRpc(3803, "protect")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__inherit_t__defined)
#define __MIG_check__Request__inherit_t__defined

mig_internal kern_return_t __MIG_check__Request__inherit_t(
	__attribute__((__unused__)) __RequestKData__inherit_t *InKP,
	__attribute__((__unused__)) __RequestUData__inherit_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__inherit_t __Request;
	typedef __RequestUData__inherit_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__inherit_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine inherit */
mig_internal novalue _Xinherit
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		vm_inherit_t new_inheritance;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__inherit_t RequestK;
	typedef __RequestUData__inherit_t __RequestU;
	typedef __ReplyKData__inherit_t ReplyK __attribute__((unused));
	typedef __ReplyUData__inherit_t ReplyU __attribute__((unused));
	typedef __Reply__inherit_t Reply __attribute__((unused));
	typedef __Request__inherit_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__inherit_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__inherit_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3804, "inherit")
	__BeforeRcvRpc(3804, "inherit")

#if	defined(__MIG_check__Request__inherit_t__defined)
	check_result = __MIG_check__Request__inherit_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__inherit_t__defined) */

	target_task = convert_port_entry_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_inherit(target_task, In0UP->address, In0UP->size, In0UP->new_inheritance);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	__AfterRcvRpc(3804, "inherit")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__read_t__defined)
#define __MIG_check__Request__read_t__defined

mig_internal kern_return_t __MIG_check__Request__read_t(
	__attribute__((__unused__)) __RequestKData__read_t *InKP,
	__attribute__((__unused__)) __RequestUData__read_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__read_t __Request;
	typedef __RequestUData__read_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__read_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine read */
mig_internal novalue _Xread
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_ut address;
		vm32_size_ut size;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__read_t RequestK;
	typedef __RequestUData__read_t __RequestU;
	typedef __ReplyKData__read_t ReplyK __attribute__((unused));
	typedef __ReplyUData__read_t ReplyU __attribute__((unused));
	typedef __Reply__read_t Reply __attribute__((unused));
	typedef __Request__read_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__read_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__read_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t dataTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t dataTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_read_t target_task;
	vm_offset_t data;

	__DeclareRcvRpc(3805, "read")
	__BeforeRcvRpc(3805, "read")

#if	defined(__MIG_check__Request__read_t__defined)
	check_result = __MIG_check__Request__read_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__read_t__defined) */

#if	UseStaticTemplates
	OutKP->data = dataTemplate;
#else	/* UseStaticTemplates */
	OutKP->data.deallocate =  FALSE;
	OutKP->data.copy = MACH_MSG_VIRTUAL_COPY;
	OutKP->data.pad1 = 0;
	OutKP->data.type = MACH_MSG_OOL_DESCRIPTOR;
#if defined(KERNEL) && !defined(__LP64__)
	OutKP->data.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	target_task = convert_port_to_map_read(InKP->Head.msgh_request_port);

	RetCode = vm32_read(target_task, In0UP->address, In0UP->size, &data, &OutUP->dataCnt);
	vm_map_read_deallocate(target_task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */
	OutKP->data.address = (void *)data;
	OutKP->data.size = OutUP->dataCnt;


	OutUP->NDR = NDR_record;


	OutKP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	OutKP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3805, "read")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__read_list_t__defined)
#define __MIG_check__Request__read_list_t__defined

mig_internal kern_return_t __MIG_check__Request__read_list_t(
	__attribute__((__unused__)) __RequestKData__read_list_t *InKP,
	__attribute__((__unused__)) __RequestUData__read_list_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__read_list_t __Request;
	typedef __RequestUData__read_list_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__read_list_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine read_list */
mig_internal novalue _Xread_list
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_read_entry_t data_list;
		natural_t count;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__read_list_t RequestK;
	typedef __RequestUData__read_list_t __RequestU;
	typedef __ReplyKData__read_list_t ReplyK __attribute__((unused));
	typedef __ReplyUData__read_list_t ReplyU __attribute__((unused));
	typedef __Reply__read_list_t Reply __attribute__((unused));
	typedef __Request__read_list_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__read_list_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__read_list_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_read_t target_task;

	__DeclareRcvRpc(3806, "read_list")
	__BeforeRcvRpc(3806, "read_list")

#if	defined(__MIG_check__Request__read_list_t__defined)
	check_result = __MIG_check__Request__read_list_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__read_list_t__defined) */

	target_task = convert_port_to_map_read(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_read_list(target_task, In0UP->data_list, In0UP->count);
	vm_map_read_deallocate(target_task);
	if (OutUP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, OutUP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	{   typedef struct { char data[2048]; } *sp;
	    * (sp) OutUP->data_list = * (sp) In0UP->data_list;
	}

	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3806, "read_list")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__write_t__defined)
#define __MIG_check__Request__write_t__defined

mig_internal kern_return_t __MIG_check__Request__write_t(
	__attribute__((__unused__)) __RequestKData__write_t *InKP,
	__attribute__((__unused__)) __RequestUData__write_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__write_t __Request;
	typedef __RequestUData__write_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if (!(InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->msgh_body.msgh_descriptor_count != 1) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

#if	__MigTypeCheck
	if (InKP->data.type != MACH_MSG_OOL_DESCRIPTOR)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

#if __MigTypeCheck
	if (InKP->data.size != In0UP->dataCnt)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__write_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine write */
mig_internal novalue _Xwrite
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_ut address;
		mach_msg_type_number_t dataCnt;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__write_t RequestK;
	typedef __RequestUData__write_t __RequestU;
	typedef __ReplyKData__write_t ReplyK __attribute__((unused));
	typedef __ReplyUData__write_t ReplyU __attribute__((unused));
	typedef __Reply__write_t Reply __attribute__((unused));
	typedef __Request__write_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__write_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__write_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3807, "write")
	__BeforeRcvRpc(3807, "write")

#if	defined(__MIG_check__Request__write_t__defined)
	check_result = __MIG_check__Request__write_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__write_t__defined) */

	target_task = convert_port_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_write(target_task, In0UP->address, (vm_offset_t)(InKP->data.address), InKP->data.size);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	__AfterRcvRpc(3807, "write")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__copy_t__defined)
#define __MIG_check__Request__copy_t__defined

mig_internal kern_return_t __MIG_check__Request__copy_t(
	__attribute__((__unused__)) __RequestKData__copy_t *InKP,
	__attribute__((__unused__)) __RequestUData__copy_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__copy_t __Request;
	typedef __RequestUData__copy_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__copy_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine copy */
mig_internal novalue _Xcopy
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_ut source_address;
		vm32_size_ut size;
		vm32_address_ut dest_address;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__copy_t RequestK;
	typedef __RequestUData__copy_t __RequestU;
	typedef __ReplyKData__copy_t ReplyK __attribute__((unused));
	typedef __ReplyUData__copy_t ReplyU __attribute__((unused));
	typedef __Reply__copy_t Reply __attribute__((unused));
	typedef __Request__copy_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__copy_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__copy_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3808, "copy")
	__BeforeRcvRpc(3808, "copy")

#if	defined(__MIG_check__Request__copy_t__defined)
	check_result = __MIG_check__Request__copy_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__copy_t__defined) */

	target_task = convert_port_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_copy(target_task, In0UP->source_address, In0UP->size, In0UP->dest_address);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	__AfterRcvRpc(3808, "copy")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__read_overwrite_t__defined)
#define __MIG_check__Request__read_overwrite_t__defined

mig_internal kern_return_t __MIG_check__Request__read_overwrite_t(
	__attribute__((__unused__)) __RequestKData__read_overwrite_t *InKP,
	__attribute__((__unused__)) __RequestUData__read_overwrite_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__read_overwrite_t __Request;
	typedef __RequestUData__read_overwrite_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__read_overwrite_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine read_overwrite */
mig_internal novalue _Xread_overwrite
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_ut address;
		vm32_size_ut size;
		vm32_address_ut data;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__read_overwrite_t RequestK;
	typedef __RequestUData__read_overwrite_t __RequestU;
	typedef __ReplyKData__read_overwrite_t ReplyK __attribute__((unused));
	typedef __ReplyUData__read_overwrite_t ReplyU __attribute__((unused));
	typedef __Reply__read_overwrite_t Reply __attribute__((unused));
	typedef __Request__read_overwrite_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__read_overwrite_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__read_overwrite_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_read_t target_task;

	__DeclareRcvRpc(3809, "read_overwrite")
	__BeforeRcvRpc(3809, "read_overwrite")

#if	defined(__MIG_check__Request__read_overwrite_t__defined)
	check_result = __MIG_check__Request__read_overwrite_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__read_overwrite_t__defined) */

	target_task = convert_port_to_map_read(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_read_overwrite(target_task, In0UP->address, In0UP->size, In0UP->data, &OutUP->outsize);
	vm_map_read_deallocate(target_task);
	if (OutUP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, OutUP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3809, "read_overwrite")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__msync_t__defined)
#define __MIG_check__Request__msync_t__defined

mig_internal kern_return_t __MIG_check__Request__msync_t(
	__attribute__((__unused__)) __RequestKData__msync_t *InKP,
	__attribute__((__unused__)) __RequestUData__msync_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__msync_t __Request;
	typedef __RequestUData__msync_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__msync_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine msync */
mig_internal novalue _Xmsync
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		vm_sync_t sync_flags;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__msync_t RequestK;
	typedef __RequestUData__msync_t __RequestU;
	typedef __ReplyKData__msync_t ReplyK __attribute__((unused));
	typedef __ReplyUData__msync_t ReplyU __attribute__((unused));
	typedef __Reply__msync_t Reply __attribute__((unused));
	typedef __Request__msync_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__msync_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__msync_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3810, "msync")
	__BeforeRcvRpc(3810, "msync")

#if	defined(__MIG_check__Request__msync_t__defined)
	check_result = __MIG_check__Request__msync_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__msync_t__defined) */

	target_task = convert_port_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_msync(target_task, In0UP->address, In0UP->size, In0UP->sync_flags);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	__AfterRcvRpc(3810, "msync")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__behavior_set_t__defined)
#define __MIG_check__Request__behavior_set_t__defined

mig_internal kern_return_t __MIG_check__Request__behavior_set_t(
	__attribute__((__unused__)) __RequestKData__behavior_set_t *InKP,
	__attribute__((__unused__)) __RequestUData__behavior_set_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__behavior_set_t __Request;
	typedef __RequestUData__behavior_set_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__behavior_set_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine behavior_set */
mig_internal novalue _Xbehavior_set
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		vm_behavior_t new_behavior;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__behavior_set_t RequestK;
	typedef __RequestUData__behavior_set_t __RequestU;
	typedef __ReplyKData__behavior_set_t ReplyK __attribute__((unused));
	typedef __ReplyUData__behavior_set_t ReplyU __attribute__((unused));
	typedef __Reply__behavior_set_t Reply __attribute__((unused));
	typedef __Request__behavior_set_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__behavior_set_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__behavior_set_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3811, "behavior_set")
	__BeforeRcvRpc(3811, "behavior_set")

#if	defined(__MIG_check__Request__behavior_set_t__defined)
	check_result = __MIG_check__Request__behavior_set_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__behavior_set_t__defined) */

	target_task = convert_port_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_behavior_set(target_task, In0UP->address, In0UP->size, In0UP->new_behavior);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	__AfterRcvRpc(3811, "behavior_set")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__map_t__defined)
#define __MIG_check__Request__map_t__defined

mig_internal kern_return_t __MIG_check__Request__map_t(
	__attribute__((__unused__)) __RequestKData__map_t *InKP,
	__attribute__((__unused__)) __RequestUData__map_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__map_t __Request;
	typedef __RequestUData__map_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if (!(InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->msgh_body.msgh_descriptor_count != 1) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

#if	__MigTypeCheck
	if (InKP->object.type != MACH_MSG_PORT_DESCRIPTOR ||
	    InKP->object.disposition != 17)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__map_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine map */
mig_internal novalue _Xmap
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_ut address;
		vm32_size_ut size;
		vm32_address_ut mask;
		int flags;
		vm32_offset_ut offset;
		boolean_t copy;
		vm_prot_ut cur_protection;
		vm_prot_ut max_protection;
		vm_inherit_ut inheritance;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__map_t RequestK;
	typedef __RequestUData__map_t __RequestU;
	typedef __ReplyKData__map_t ReplyK __attribute__((unused));
	typedef __ReplyUData__map_t ReplyU __attribute__((unused));
	typedef __Reply__map_t Reply __attribute__((unused));
	typedef __Request__map_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__map_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__map_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3812, "map")
	__BeforeRcvRpc(3812, "map")

#if	defined(__MIG_check__Request__map_t__defined)
	check_result = __MIG_check__Request__map_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__map_t__defined) */

	target_task = convert_port_entry_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_map(target_task, &In0UP->address, In0UP->size, In0UP->mask, In0UP->flags, null_conversion(InKP->object.name), In0UP->offset, In0UP->copy, In0UP->cur_protection, In0UP->max_protection, In0UP->inheritance);
	vm_map_deallocate(target_task);
	if (OutUP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, OutUP->RetCode);
	}
#if	__MigKernelSpecificCode
	ipc_port_release_send((ipc_port_t)InKP->object.name);
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	OutUP->address = In0UP->address;

	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3812, "map")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__machine_attribute_t__defined)
#define __MIG_check__Request__machine_attribute_t__defined

mig_internal kern_return_t __MIG_check__Request__machine_attribute_t(
	__attribute__((__unused__)) __RequestKData__machine_attribute_t *InKP,
	__attribute__((__unused__)) __RequestUData__machine_attribute_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__machine_attribute_t __Request;
	typedef __RequestUData__machine_attribute_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__machine_attribute_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine machine_attribute */
mig_internal novalue _Xmachine_attribute
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		vm_machine_attribute_t attribute;
		vm_machine_attribute_val_t value;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__machine_attribute_t RequestK;
	typedef __RequestUData__machine_attribute_t __RequestU;
	typedef __ReplyKData__machine_attribute_t ReplyK __attribute__((unused));
	typedef __ReplyUData__machine_attribute_t ReplyU __attribute__((unused));
	typedef __Reply__machine_attribute_t Reply __attribute__((unused));
	typedef __Request__machine_attribute_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__machine_attribute_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__machine_attribute_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3813, "machine_attribute")
	__BeforeRcvRpc(3813, "machine_attribute")

#if	defined(__MIG_check__Request__machine_attribute_t__defined)
	check_result = __MIG_check__Request__machine_attribute_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__machine_attribute_t__defined) */

	target_task = convert_port_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_machine_attribute(target_task, In0UP->address, In0UP->size, In0UP->attribute, &In0UP->value);
	vm_map_deallocate(target_task);
	if (OutUP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, OutUP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	OutUP->value = In0UP->value;

	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3813, "machine_attribute")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__remap_t__defined)
#define __MIG_check__Request__remap_t__defined

mig_internal kern_return_t __MIG_check__Request__remap_t(
	__attribute__((__unused__)) __RequestKData__remap_t *InKP,
	__attribute__((__unused__)) __RequestUData__remap_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__remap_t __Request;
	typedef __RequestUData__remap_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if (!(InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->msgh_body.msgh_descriptor_count != 1) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

#if	__MigTypeCheck
	if (InKP->src_task.type != MACH_MSG_PORT_DESCRIPTOR ||
	    InKP->src_task.disposition != 17)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__remap_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine remap */
mig_internal novalue _Xremap
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_ut target_address;
		vm32_size_ut size;
		vm32_address_ut mask;
		boolean_t anywhere;
		vm32_address_ut src_address;
		boolean_t copy;
		vm_inherit_ut inheritance;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__remap_t RequestK;
	typedef __RequestUData__remap_t __RequestU;
	typedef __ReplyKData__remap_t ReplyK __attribute__((unused));
	typedef __ReplyUData__remap_t ReplyU __attribute__((unused));
	typedef __Reply__remap_t Reply __attribute__((unused));
	typedef __Request__remap_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__remap_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__remap_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;
	vm_map_t src_task;

	__DeclareRcvRpc(3814, "remap")
	__BeforeRcvRpc(3814, "remap")

#if	defined(__MIG_check__Request__remap_t__defined)
	check_result = __MIG_check__Request__remap_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__remap_t__defined) */

	target_task = convert_port_to_map(InKP->Head.msgh_request_port);

	src_task = convert_port_to_map(InKP->src_task.name);

	OutUP->RetCode = vm32_remap(target_task, &In0UP->target_address, In0UP->size, In0UP->mask, In0UP->anywhere, src_task, In0UP->src_address, In0UP->copy, &OutUP->cur_protection, &OutUP->max_protection, In0UP->inheritance);
	vm_map_deallocate(src_task);
	vm_map_deallocate(target_task);
	if (OutUP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, OutUP->RetCode);
	}
#if	__MigKernelSpecificCode
	ipc_port_release_send((ipc_port_t)InKP->src_task.name);
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	OutUP->target_address = In0UP->target_address;

	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3814, "remap")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request___task_wire_t__defined)
#define __MIG_check__Request___task_wire_t__defined

mig_internal kern_return_t __MIG_check__Request___task_wire_t(
	__attribute__((__unused__)) __RequestKData___task_wire_t *InKP,
	__attribute__((__unused__)) __RequestUData___task_wire_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request___task_wire_t __Request;
	typedef __RequestUData___task_wire_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request___task_wire_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine _task_wire */
mig_internal novalue _X_task_wire
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		boolean_t must_wire;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData___task_wire_t RequestK;
	typedef __RequestUData___task_wire_t __RequestU;
	typedef __ReplyKData___task_wire_t ReplyK __attribute__((unused));
	typedef __ReplyUData___task_wire_t ReplyU __attribute__((unused));
	typedef __Reply___task_wire_t Reply __attribute__((unused));
	typedef __Request___task_wire_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request___task_wire_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request___task_wire_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3815, "_task_wire")
	__BeforeRcvRpc(3815, "_task_wire")

#if	defined(__MIG_check__Request___task_wire_t__defined)
	check_result = __MIG_check__Request___task_wire_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request___task_wire_t__defined) */

	target_task = convert_port_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32__task_wire(target_task, In0UP->must_wire);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	__AfterRcvRpc(3815, "_task_wire")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__make_memory_entry_t__defined)
#define __MIG_check__Request__make_memory_entry_t__defined

mig_internal kern_return_t __MIG_check__Request__make_memory_entry_t(
	__attribute__((__unused__)) __RequestKData__make_memory_entry_t *InKP,
	__attribute__((__unused__)) __RequestUData__make_memory_entry_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__make_memory_entry_t __Request;
	typedef __RequestUData__make_memory_entry_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if (!(InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->msgh_body.msgh_descriptor_count != 1) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

#if	__MigTypeCheck
	if (InKP->parent_entry.type != MACH_MSG_PORT_DESCRIPTOR ||
	    InKP->parent_entry.disposition != 17)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__make_memory_entry_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine make_memory_entry */
mig_internal novalue _Xmake_memory_entry
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_size_ut size;
		vm32_offset_ut offset;
		vm_prot_ut permission;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__make_memory_entry_t RequestK;
	typedef __RequestUData__make_memory_entry_t __RequestU;
	typedef __ReplyKData__make_memory_entry_t ReplyK __attribute__((unused));
	typedef __ReplyUData__make_memory_entry_t ReplyU __attribute__((unused));
	typedef __Reply__make_memory_entry_t Reply __attribute__((unused));
	typedef __Request__make_memory_entry_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__make_memory_entry_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__make_memory_entry_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_handleTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_handleTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_t target_task;
	mem_entry_name_port_t object_handle;

	__DeclareRcvRpc(3816, "make_memory_entry")
	__BeforeRcvRpc(3816, "make_memory_entry")

#if	defined(__MIG_check__Request__make_memory_entry_t__defined)
	check_result = __MIG_check__Request__make_memory_entry_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__make_memory_entry_t__defined) */

#if	UseStaticTemplates
	OutKP->object_handle = object_handleTemplate;
#else	/* UseStaticTemplates */
#if __MigKernelSpecificCode
	OutKP->object_handle.disposition = 17;
#else
	OutKP->object_handle.disposition = 17;
#endif /* __MigKernelSpecificCode */
#if !(defined(KERNEL) && defined(__LP64__))
	OutKP->object_handle.pad1 = 0;
#endif
	OutKP->object_handle.pad2 = 0;
	OutKP->object_handle.type = MACH_MSG_PORT_DESCRIPTOR;
#if defined(KERNEL)
	OutKP->object_handle.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	target_task = convert_port_to_map(InKP->Head.msgh_request_port);

	RetCode = vm32_make_memory_entry(target_task, &In0UP->size, In0UP->offset, In0UP->permission, &object_handle, null_conversion(InKP->parent_entry.name));
	vm_map_deallocate(target_task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, RetCode);
	}
#if	__MigKernelSpecificCode
	ipc_port_release_send((ipc_port_t)InKP->parent_entry.name);
#endif /* __MigKernelSpecificCode */
	OutKP->object_handle.name = (mach_port_t)null_conversion(object_handle);


	OutUP->NDR = NDR_record;


	OutUP->size = In0UP->size;

	OutKP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	OutKP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3816, "make_memory_entry")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__map_page_query_t__defined)
#define __MIG_check__Request__map_page_query_t__defined

mig_internal kern_return_t __MIG_check__Request__map_page_query_t(
	__attribute__((__unused__)) __RequestKData__map_page_query_t *InKP,
	__attribute__((__unused__)) __RequestUData__map_page_query_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__map_page_query_t __Request;
	typedef __RequestUData__map_page_query_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__map_page_query_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine map_page_query */
mig_internal novalue _Xmap_page_query
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_offset_t offset;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__map_page_query_t RequestK;
	typedef __RequestUData__map_page_query_t __RequestU;
	typedef __ReplyKData__map_page_query_t ReplyK __attribute__((unused));
	typedef __ReplyUData__map_page_query_t ReplyU __attribute__((unused));
	typedef __Reply__map_page_query_t Reply __attribute__((unused));
	typedef __Request__map_page_query_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__map_page_query_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__map_page_query_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_read_t target_map;

	__DeclareRcvRpc(3817, "map_page_query")
	__BeforeRcvRpc(3817, "map_page_query")

#if	defined(__MIG_check__Request__map_page_query_t__defined)
	check_result = __MIG_check__Request__map_page_query_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__map_page_query_t__defined) */

	target_map = convert_port_to_map_read(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_map_page_query(target_map, In0UP->offset, &OutUP->disposition, &OutUP->ref_count);
	vm_map_read_deallocate(target_map);
	if (OutUP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, OutUP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3817, "map_page_query")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__region_info_t__defined)
#define __MIG_check__Request__region_info_t__defined

mig_internal kern_return_t __MIG_check__Request__region_info_t(
	__attribute__((__unused__)) __RequestKData__region_info_t *InKP,
	__attribute__((__unused__)) __RequestUData__region_info_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__region_info_t __Request;
	typedef __RequestUData__region_info_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__region_info_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine region_info */
mig_internal novalue _Xregion_info
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_t address;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__region_info_t RequestK;
	typedef __RequestUData__region_info_t __RequestU;
	typedef __ReplyKData__region_info_t ReplyK __attribute__((unused));
	typedef __ReplyUData__region_info_t ReplyU __attribute__((unused));
	typedef __Reply__region_info_t Reply __attribute__((unused));
	typedef __Request__region_info_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__region_info_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__region_info_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t objectsTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t objectsTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_read_t task;
	vm_info_object_array_t objects;

	__DeclareRcvRpc(3818, "region_info")
	__BeforeRcvRpc(3818, "region_info")

#if	defined(__MIG_check__Request__region_info_t__defined)
	check_result = __MIG_check__Request__region_info_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__region_info_t__defined) */

#if	UseStaticTemplates
	OutKP->objects = objectsTemplate;
#else	/* UseStaticTemplates */
	OutKP->objects.deallocate =  FALSE;
	OutKP->objects.copy = MACH_MSG_VIRTUAL_COPY;
	OutKP->objects.pad1 = 0;
	OutKP->objects.type = MACH_MSG_OOL_DESCRIPTOR;
#if defined(KERNEL) && !defined(__LP64__)
	OutKP->objects.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	task = convert_port_to_map_read(InKP->Head.msgh_request_port);

	RetCode = vm32_region_info(task, In0UP->address, &OutUP->region, &objects, &OutUP->objectsCnt);
	vm_map_read_deallocate(task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */
	OutKP->objects.address = (void *)objects;
	OutKP->objects.size = OutUP->objectsCnt * 84;


	OutUP->NDR = NDR_record;


	OutKP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	OutKP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3818, "region_info")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__mapped_pages_info_t__defined)
#define __MIG_check__Request__mapped_pages_info_t__defined

mig_internal kern_return_t __MIG_check__Request__mapped_pages_info_t(
	__attribute__((__unused__)) __RequestKData__mapped_pages_info_t *InKP,
	__attribute__((__unused__)) __RequestUData__mapped_pages_info_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__mapped_pages_info_t __Request;
	typedef __RequestUData__mapped_pages_info_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__mapped_pages_info_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine mapped_pages_info */
mig_internal novalue _Xmapped_pages_info
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__mapped_pages_info_t RequestK;
	typedef __RequestUData__mapped_pages_info_t __RequestU;
	typedef __ReplyKData__mapped_pages_info_t ReplyK __attribute__((unused));
	typedef __ReplyUData__mapped_pages_info_t ReplyU __attribute__((unused));
	typedef __Reply__mapped_pages_info_t Reply __attribute__((unused));
	typedef __Request__mapped_pages_info_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__mapped_pages_info_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__mapped_pages_info_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t pagesTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t pagesTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_read_t task;
	page_address_array_t pages;

	__DeclareRcvRpc(3819, "mapped_pages_info")
	__BeforeRcvRpc(3819, "mapped_pages_info")

#if	defined(__MIG_check__Request__mapped_pages_info_t__defined)
	check_result = __MIG_check__Request__mapped_pages_info_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__mapped_pages_info_t__defined) */

#if	UseStaticTemplates
	OutKP->pages = pagesTemplate;
#else	/* UseStaticTemplates */
	OutKP->pages.deallocate =  FALSE;
	OutKP->pages.copy = MACH_MSG_VIRTUAL_COPY;
	OutKP->pages.pad1 = 0;
	OutKP->pages.type = MACH_MSG_OOL_DESCRIPTOR;
#if defined(KERNEL) && !defined(__LP64__)
	OutKP->pages.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	task = convert_port_to_map_read(InKP->Head.msgh_request_port);

	RetCode = vm32_mapped_pages_info(task, &pages, &OutUP->pagesCnt);
	vm_map_read_deallocate(task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */
	OutKP->pages.address = (void *)pages;
	OutKP->pages.size = OutUP->pagesCnt * 4;


	OutUP->NDR = NDR_record;


	OutKP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	OutKP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3819, "mapped_pages_info")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__region_recurse_t__defined)
#define __MIG_check__Request__region_recurse_t__defined

mig_internal kern_return_t __MIG_check__Request__region_recurse_t(
	__attribute__((__unused__)) __RequestKData__region_recurse_t *InKP,
	__attribute__((__unused__)) __RequestUData__region_recurse_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__region_recurse_t __Request;
	typedef __RequestUData__region_recurse_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__region_recurse_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine region_recurse */
mig_internal novalue _Xregion_recurse
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_t address;
		natural_t nesting_depth;
		mach_msg_type_number_t infoCnt;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__region_recurse_t RequestK;
	typedef __RequestUData__region_recurse_t __RequestU;
	typedef __ReplyKData__region_recurse_t ReplyK __attribute__((unused));
	typedef __ReplyUData__region_recurse_t ReplyU __attribute__((unused));
	typedef __Reply__region_recurse_t Reply __attribute__((unused));
	typedef __Request__region_recurse_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__region_recurse_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__region_recurse_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_read_t target_task;

	__DeclareRcvRpc(3821, "region_recurse")
	__BeforeRcvRpc(3821, "region_recurse")

#if	defined(__MIG_check__Request__region_recurse_t__defined)
	check_result = __MIG_check__Request__region_recurse_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__region_recurse_t__defined) */

	target_task = convert_port_to_map_read(InKP->Head.msgh_request_port);

	OutUP->infoCnt = 19;
	if (In0UP->infoCnt < OutUP->infoCnt)
		OutUP->infoCnt = In0UP->infoCnt;

	OutUP->RetCode = vm32_region_recurse(target_task, &In0UP->address, &OutUP->size, &In0UP->nesting_depth, OutUP->info, &OutUP->infoCnt);
	vm_map_read_deallocate(target_task);
	if (OutUP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, OutUP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	OutUP->address = In0UP->address;

	OutUP->nesting_depth = In0UP->nesting_depth;
	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply) - 76) + (((4 * OutUP->infoCnt)));

	__AfterRcvRpc(3821, "region_recurse")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__region_recurse_64_t__defined)
#define __MIG_check__Request__region_recurse_64_t__defined

mig_internal kern_return_t __MIG_check__Request__region_recurse_64_t(
	__attribute__((__unused__)) __RequestKData__region_recurse_64_t *InKP,
	__attribute__((__unused__)) __RequestUData__region_recurse_64_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__region_recurse_64_t __Request;
	typedef __RequestUData__region_recurse_64_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__region_recurse_64_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine region_recurse_64 */
mig_internal novalue _Xregion_recurse_64
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_t address;
		natural_t nesting_depth;
		mach_msg_type_number_t infoCnt;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__region_recurse_64_t RequestK;
	typedef __RequestUData__region_recurse_64_t __RequestU;
	typedef __ReplyKData__region_recurse_64_t ReplyK __attribute__((unused));
	typedef __ReplyUData__region_recurse_64_t ReplyU __attribute__((unused));
	typedef __Reply__region_recurse_64_t Reply __attribute__((unused));
	typedef __Request__region_recurse_64_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__region_recurse_64_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__region_recurse_64_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_read_t target_task;

	__DeclareRcvRpc(3822, "region_recurse_64")
	__BeforeRcvRpc(3822, "region_recurse_64")

#if	defined(__MIG_check__Request__region_recurse_64_t__defined)
	check_result = __MIG_check__Request__region_recurse_64_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__region_recurse_64_t__defined) */

	target_task = convert_port_to_map_read(InKP->Head.msgh_request_port);

	OutUP->infoCnt = 19;
	if (In0UP->infoCnt < OutUP->infoCnt)
		OutUP->infoCnt = In0UP->infoCnt;

	OutUP->RetCode = vm32_region_recurse_64(target_task, &In0UP->address, &OutUP->size, &In0UP->nesting_depth, OutUP->info, &OutUP->infoCnt);
	vm_map_read_deallocate(target_task);
	if (OutUP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, OutUP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	OutUP->address = In0UP->address;

	OutUP->nesting_depth = In0UP->nesting_depth;
	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply) - 76) + (((4 * OutUP->infoCnt)));

	__AfterRcvRpc(3822, "region_recurse_64")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__region_info_64_t__defined)
#define __MIG_check__Request__region_info_64_t__defined

mig_internal kern_return_t __MIG_check__Request__region_info_64_t(
	__attribute__((__unused__)) __RequestKData__region_info_64_t *InKP,
	__attribute__((__unused__)) __RequestUData__region_info_64_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__region_info_64_t __Request;
	typedef __RequestUData__region_info_64_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__region_info_64_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine region_info_64 */
mig_internal novalue _Xregion_info_64
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_t address;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__region_info_64_t RequestK;
	typedef __RequestUData__region_info_64_t __RequestU;
	typedef __ReplyKData__region_info_64_t ReplyK __attribute__((unused));
	typedef __ReplyUData__region_info_64_t ReplyU __attribute__((unused));
	typedef __Reply__region_info_64_t Reply __attribute__((unused));
	typedef __Request__region_info_64_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__region_info_64_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__region_info_64_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t objectsTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t objectsTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_read_t task;
	vm_info_object_array_t objects;

	__DeclareRcvRpc(3823, "region_info_64")
	__BeforeRcvRpc(3823, "region_info_64")

#if	defined(__MIG_check__Request__region_info_64_t__defined)
	check_result = __MIG_check__Request__region_info_64_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__region_info_64_t__defined) */

#if	UseStaticTemplates
	OutKP->objects = objectsTemplate;
#else	/* UseStaticTemplates */
	OutKP->objects.deallocate =  FALSE;
	OutKP->objects.copy = MACH_MSG_VIRTUAL_COPY;
	OutKP->objects.pad1 = 0;
	OutKP->objects.type = MACH_MSG_OOL_DESCRIPTOR;
#if defined(KERNEL) && !defined(__LP64__)
	OutKP->objects.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	task = convert_port_to_map_read(InKP->Head.msgh_request_port);

	RetCode = vm32_region_info_64(task, In0UP->address, &OutUP->region, &objects, &OutUP->objectsCnt);
	vm_map_read_deallocate(task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */
	OutKP->objects.address = (void *)objects;
	OutKP->objects.size = OutUP->objectsCnt * 84;


	OutUP->NDR = NDR_record;


	OutKP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	OutKP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3823, "region_info_64")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__region_64_t__defined)
#define __MIG_check__Request__region_64_t__defined

mig_internal kern_return_t __MIG_check__Request__region_64_t(
	__attribute__((__unused__)) __RequestKData__region_64_t *InKP,
	__attribute__((__unused__)) __RequestUData__region_64_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__region_64_t __Request;
	typedef __RequestUData__region_64_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__region_64_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine region_64 */
mig_internal novalue _Xregion_64
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_t address;
		vm_region_flavor_t flavor;
		mach_msg_type_number_t infoCnt;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__region_64_t RequestK;
	typedef __RequestUData__region_64_t __RequestU;
	typedef __ReplyKData__region_64_t ReplyK __attribute__((unused));
	typedef __ReplyUData__region_64_t ReplyU __attribute__((unused));
	typedef __Reply__region_64_t Reply __attribute__((unused));
	typedef __Request__region_64_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__region_64_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__region_64_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_nameTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_nameTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_read_t target_task;
	mach_port_t object_name;

	__DeclareRcvRpc(3824, "region_64")
	__BeforeRcvRpc(3824, "region_64")

#if	defined(__MIG_check__Request__region_64_t__defined)
	check_result = __MIG_check__Request__region_64_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__region_64_t__defined) */

#if	UseStaticTemplates
	OutKP->object_name = object_nameTemplate;
#else	/* UseStaticTemplates */
#if __MigKernelSpecificCode
	OutKP->object_name.disposition = 17;
#else
	OutKP->object_name.disposition = 17;
#endif /* __MigKernelSpecificCode */
#if !(defined(KERNEL) && defined(__LP64__))
	OutKP->object_name.pad1 = 0;
#endif
	OutKP->object_name.pad2 = 0;
	OutKP->object_name.type = MACH_MSG_PORT_DESCRIPTOR;
#if defined(KERNEL)
	OutKP->object_name.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	target_task = convert_port_to_map_read(InKP->Head.msgh_request_port);

	OutUP->infoCnt = 10;
	if (In0UP->infoCnt < OutUP->infoCnt)
		OutUP->infoCnt = In0UP->infoCnt;

	RetCode = vm32_region_64(target_task, &In0UP->address, &OutUP->size, In0UP->flavor, OutUP->info, &OutUP->infoCnt, &object_name);
	vm_map_read_deallocate(target_task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */
	OutKP->object_name.name = object_name;


	OutUP->NDR = NDR_record;


	OutUP->address = In0UP->address;
	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply) - 40) + (((4 * OutUP->infoCnt)));

	OutKP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutKP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3824, "region_64")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__make_memory_entry_64_t__defined)
#define __MIG_check__Request__make_memory_entry_64_t__defined

mig_internal kern_return_t __MIG_check__Request__make_memory_entry_64_t(
	__attribute__((__unused__)) __RequestKData__make_memory_entry_64_t *InKP,
	__attribute__((__unused__)) __RequestUData__make_memory_entry_64_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__make_memory_entry_64_t __Request;
	typedef __RequestUData__make_memory_entry_64_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if (!(InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->msgh_body.msgh_descriptor_count != 1) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

#if	__MigTypeCheck
	if (InKP->parent_entry.type != MACH_MSG_PORT_DESCRIPTOR ||
	    InKP->parent_entry.disposition != 17)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__make_memory_entry_64_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine make_memory_entry_64 */
mig_internal novalue _Xmake_memory_entry_64
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		memory_object_size_ut size;
		memory_object_offset_ut offset;
		vm_prot_ut permission;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__make_memory_entry_64_t RequestK;
	typedef __RequestUData__make_memory_entry_64_t __RequestU;
	typedef __ReplyKData__make_memory_entry_64_t ReplyK __attribute__((unused));
	typedef __ReplyUData__make_memory_entry_64_t ReplyU __attribute__((unused));
	typedef __Reply__make_memory_entry_64_t Reply __attribute__((unused));
	typedef __Request__make_memory_entry_64_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__make_memory_entry_64_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__make_memory_entry_64_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_handleTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_handleTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_t target_task;
	mach_port_t object_handle;

	__DeclareRcvRpc(3825, "make_memory_entry_64")
	__BeforeRcvRpc(3825, "make_memory_entry_64")

#if	defined(__MIG_check__Request__make_memory_entry_64_t__defined)
	check_result = __MIG_check__Request__make_memory_entry_64_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__make_memory_entry_64_t__defined) */

#if	UseStaticTemplates
	OutKP->object_handle = object_handleTemplate;
#else	/* UseStaticTemplates */
#if __MigKernelSpecificCode
	OutKP->object_handle.disposition = 17;
#else
	OutKP->object_handle.disposition = 17;
#endif /* __MigKernelSpecificCode */
#if !(defined(KERNEL) && defined(__LP64__))
	OutKP->object_handle.pad1 = 0;
#endif
	OutKP->object_handle.pad2 = 0;
	OutKP->object_handle.type = MACH_MSG_PORT_DESCRIPTOR;
#if defined(KERNEL)
	OutKP->object_handle.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	target_task = convert_port_to_map(InKP->Head.msgh_request_port);

	RetCode = vm32_make_memory_entry_64(target_task, &In0UP->size, In0UP->offset, In0UP->permission, &object_handle, null_conversion(InKP->parent_entry.name));
	vm_map_deallocate(target_task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, RetCode);
	}
#if	__MigKernelSpecificCode
	ipc_port_release_send((ipc_port_t)InKP->parent_entry.name);
#endif /* __MigKernelSpecificCode */
	OutKP->object_handle.name = object_handle;


	OutUP->NDR = NDR_record;


	OutUP->size = In0UP->size;

	OutKP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	OutKP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3825, "make_memory_entry_64")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__map_64_t__defined)
#define __MIG_check__Request__map_64_t__defined

mig_internal kern_return_t __MIG_check__Request__map_64_t(
	__attribute__((__unused__)) __RequestKData__map_64_t *InKP,
	__attribute__((__unused__)) __RequestUData__map_64_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__map_64_t __Request;
	typedef __RequestUData__map_64_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if (!(InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->msgh_body.msgh_descriptor_count != 1) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

#if	__MigTypeCheck
	if (InKP->object.type != MACH_MSG_PORT_DESCRIPTOR ||
	    InKP->object.disposition != 17)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__map_64_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine map_64 */
mig_internal novalue _Xmap_64
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_ut address;
		vm32_size_ut size;
		vm32_address_ut mask;
		int flags;
		memory_object_offset_ut offset;
		boolean_t copy;
		vm_prot_ut cur_protection;
		vm_prot_ut max_protection;
		vm_inherit_ut inheritance;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__map_64_t RequestK;
	typedef __RequestUData__map_64_t __RequestU;
	typedef __ReplyKData__map_64_t ReplyK __attribute__((unused));
	typedef __ReplyUData__map_64_t ReplyU __attribute__((unused));
	typedef __Reply__map_64_t Reply __attribute__((unused));
	typedef __Request__map_64_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__map_64_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__map_64_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3826, "map_64")
	__BeforeRcvRpc(3826, "map_64")

#if	defined(__MIG_check__Request__map_64_t__defined)
	check_result = __MIG_check__Request__map_64_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__map_64_t__defined) */

	target_task = convert_port_entry_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_map_64(target_task, &In0UP->address, In0UP->size, In0UP->mask, In0UP->flags, null_conversion(InKP->object.name), In0UP->offset, In0UP->copy, In0UP->cur_protection, In0UP->max_protection, In0UP->inheritance);
	vm_map_deallocate(target_task);
	if (OutUP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, OutUP->RetCode);
	}
#if	__MigKernelSpecificCode
	ipc_port_release_send((ipc_port_t)InKP->object.name);
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	OutUP->address = In0UP->address;

	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3826, "map_64")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__purgable_control_t__defined)
#define __MIG_check__Request__purgable_control_t__defined

mig_internal kern_return_t __MIG_check__Request__purgable_control_t(
	__attribute__((__unused__)) __RequestKData__purgable_control_t *InKP,
	__attribute__((__unused__)) __RequestUData__purgable_control_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request__purgable_control_t __Request;
	typedef __RequestUData__purgable_control_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__purgable_control_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine purgable_control */
mig_internal novalue _Xpurgable_control
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		NDR_record_t NDR;
		vm32_address_t address;
		vm_purgable_t control;
		int state;
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData__purgable_control_t RequestK;
	typedef __RequestUData__purgable_control_t __RequestU;
	typedef __ReplyKData__purgable_control_t ReplyK __attribute__((unused));
	typedef __ReplyUData__purgable_control_t ReplyU __attribute__((unused));
	typedef __Reply__purgable_control_t Reply __attribute__((unused));
	typedef __Request__purgable_control_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request__purgable_control_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__purgable_control_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3830, "purgable_control")
	__BeforeRcvRpc(3830, "purgable_control")

#if	defined(__MIG_check__Request__purgable_control_t__defined)
	check_result = __MIG_check__Request__purgable_control_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request__purgable_control_t__defined) */

	target_task = convert_port_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32_purgable_control(target_task, In0UP->address, In0UP->control, &In0UP->state);
	vm_map_deallocate(target_task);
	if (OutUP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutKP, OutUP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	OutUP->state = In0UP->state;

	OutKP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3830, "purgable_control")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request___map_exec_lockdown_t__defined)
#define __MIG_check__Request___map_exec_lockdown_t__defined

mig_internal kern_return_t __MIG_check__Request___map_exec_lockdown_t(
	__attribute__((__unused__)) __RequestKData___map_exec_lockdown_t *InKP,
	__attribute__((__unused__)) __RequestUData___map_exec_lockdown_t *In0UP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP)
{

	typedef __Request___map_exec_lockdown_t __Request;
	typedef __RequestUData___map_exec_lockdown_t __RequestU __attribute__((unused));
#if	__MigTypeCheck
	if ((InKP->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (InKP->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request___map_exec_lockdown_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine _map_exec_lockdown */
mig_internal novalue _X_map_exec_lockdown
	(mach_msg_header_t *InHeadP, __attribute__((__unused__)) void *InDataP,
	__attribute__((__unused__)) mach_msg_max_trailer_t *InTrailerP,
	mach_msg_header_t *OutHeadP, __attribute__((__unused__)) void *OutDataP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_trailer_t trailer;
		char padding[0]; /* Avoid generating empty UData structs */
	} RequestU __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __RequestKData___map_exec_lockdown_t RequestK;
	typedef __RequestUData___map_exec_lockdown_t __RequestU;
	typedef __ReplyKData___map_exec_lockdown_t ReplyK __attribute__((unused));
	typedef __ReplyUData___map_exec_lockdown_t ReplyU __attribute__((unused));
	typedef __Reply___map_exec_lockdown_t Reply __attribute__((unused));
	typedef __Request___map_exec_lockdown_t __Request __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	RequestK *InKP = (RequestK *) InHeadP;
	RequestU *In0UP = (RequestU *) InDataP;
	ReplyK *OutKP = (ReplyK *) OutHeadP;
	ReplyU *OutUP = (ReplyU *) OutDataP;
	(void)OutUP;
#ifdef	__MIG_check__Request___map_exec_lockdown_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request___map_exec_lockdown_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3831, "_map_exec_lockdown")
	__BeforeRcvRpc(3831, "_map_exec_lockdown")

#if	defined(__MIG_check__Request___map_exec_lockdown_t__defined)
	check_result = __MIG_check__Request___map_exec_lockdown_t((RequestK *)InKP, (__RequestU *)In0UP, InTrailerP);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutKP, check_result); }
#endif	/* defined(__MIG_check__Request___map_exec_lockdown_t__defined) */

	target_task = convert_port_to_map(InKP->Head.msgh_request_port);

	OutUP->RetCode = vm32__map_exec_lockdown(target_task);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutUP->NDR = NDR_record;


	__AfterRcvRpc(3831, "_map_exec_lockdown")
}



/* Description of this kernel subsystem, for use in direct RPC */
const struct vm32_map_subsystem vm32_map_subsystem = {
	map_server_routine,
	3800,
	3832,
	(mach_msg_size_t)sizeof(union __ReplyUnion__vm32_map_subsystem),
	(vm_address_t)0,
	{
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xregion, 7, 0, 1, (mach_msg_size_t)sizeof(__Reply__region_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xallocate, 4, 0, 0, (mach_msg_size_t)sizeof(__Reply__allocate_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xdeallocate, 3, 0, 0, (mach_msg_size_t)sizeof(__Reply__deallocate_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xprotect, 5, 0, 0, (mach_msg_size_t)sizeof(__Reply__protect_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xinherit, 4, 0, 0, (mach_msg_size_t)sizeof(__Reply__inherit_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xread, 5, 0, 1, (mach_msg_size_t)sizeof(__Reply__read_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xread_list, 3, 0, 0, (mach_msg_size_t)sizeof(__Reply__read_list_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xwrite, 4, 0, 0, (mach_msg_size_t)sizeof(__Reply__write_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xcopy, 4, 0, 0, (mach_msg_size_t)sizeof(__Reply__copy_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xread_overwrite, 5, 0, 0, (mach_msg_size_t)sizeof(__Reply__read_overwrite_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xmsync, 4, 0, 0, (mach_msg_size_t)sizeof(__Reply__msync_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xbehavior_set, 4, 0, 0, (mach_msg_size_t)sizeof(__Reply__behavior_set_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xmap, 11, 0, 0, (mach_msg_size_t)sizeof(__Reply__map_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xmachine_attribute, 5, 0, 0, (mach_msg_size_t)sizeof(__Reply__machine_attribute_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xremap, 11, 0, 0, (mach_msg_size_t)sizeof(__Reply__remap_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _X_task_wire, 2, 0, 0, (mach_msg_size_t)sizeof(__Reply___task_wire_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xmake_memory_entry, 6, 0, 1, (mach_msg_size_t)sizeof(__Reply__make_memory_entry_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xmap_page_query, 4, 0, 0, (mach_msg_size_t)sizeof(__Reply__map_page_query_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xregion_info, 5, 0, 1, (mach_msg_size_t)sizeof(__Reply__region_info_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xmapped_pages_info, 3, 0, 1, (mach_msg_size_t)sizeof(__Reply__mapped_pages_info_t)},
		{0, 0, 0, 0, 0, 0},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xregion_recurse, 6, 0, 0, (mach_msg_size_t)sizeof(__Reply__region_recurse_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xregion_recurse_64, 6, 0, 0, (mach_msg_size_t)sizeof(__Reply__region_recurse_64_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xregion_info_64, 5, 0, 1, (mach_msg_size_t)sizeof(__Reply__region_info_64_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xregion_64, 7, 0, 1, (mach_msg_size_t)sizeof(__Reply__region_64_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xmake_memory_entry_64, 7, 0, 1, (mach_msg_size_t)sizeof(__Reply__make_memory_entry_64_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xmap_64, 12, 0, 0, (mach_msg_size_t)sizeof(__Reply__map_64_t)},
		{0, 0, 0, 0, 0, 0},
		{0, 0, 0, 0, 0, 0},
		{0, 0, 0, 0, 0, 0},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _Xpurgable_control, 4, 0, 0, (mach_msg_size_t)sizeof(__Reply__purgable_control_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_kern_routine_t) _X_map_exec_lockdown, 1, 0, 0, (mach_msg_size_t)sizeof(__Reply___map_exec_lockdown_t)},
	}
};

mig_external boolean_t map_server
	(mach_msg_header_t *InHeadP, void *InDataP, mach_msg_max_trailer_t *InTrailerP, mach_msg_header_t *OutHeadP, void *OutDataP)
{
	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	mig_kern_routine_t routine;

	OutHeadP->msgh_bits = MACH_MSGH_BITS(MACH_MSGH_BITS_REPLY(InHeadP->msgh_bits), 0);
	OutHeadP->msgh_remote_port = InHeadP->msgh_reply_port;
	/* Minimal size: routine() will update it if different */
	OutHeadP->msgh_size = (mach_msg_size_t)sizeof(mig_reply_error_t);
	OutHeadP->msgh_local_port = MACH_PORT_NULL;
	OutHeadP->msgh_id = InHeadP->msgh_id + 100;
	OutHeadP->msgh_reserved = 0;

	if ((InHeadP->msgh_id > 3831) || (InHeadP->msgh_id < 3800) ||
	    ((routine = vm32_map_subsystem.kroutine[InHeadP->msgh_id - 3800].kstub_routine) == 0)) {
		((mig_reply_error_t *)OutHeadP)->NDR = NDR_record;
		((mig_reply_error_t *)OutHeadP)->RetCode = MIG_BAD_ID;
		return FALSE;
	}
	(*routine) (InHeadP, InDataP, InTrailerP, OutHeadP, OutDataP);
	return TRUE;
}

mig_external mig_kern_routine_t map_server_routine
	(mach_msg_header_t *InHeadP)
{
	int msgh_id;

	msgh_id = InHeadP->msgh_id - 3800;

	if ((msgh_id > 31) || (msgh_id < 0))
		return 0;

	return vm32_map_subsystem.kroutine[msgh_id].kstub_routine;
}
